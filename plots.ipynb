{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 45) (500, 1)\n",
      "(400, 45) (400, 1)\n",
      "(100, 45) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from logistic_regression.GrdDscntQuant import grdescentquant\n",
    "from logistic_regression.normal_logistic import normallogistic\n",
    "from logistic_regression.GrdDscnt import grdescentnormal\n",
    "from logistic_regression.quant_logistic import quant_logistic\n",
    "from logistic_regression.GrdDscntUncoded import grdescentuncoded\n",
    "from logistic_regression.uncoded_logistic import  uncoded_logistic\n",
    "from quantization.quantize import quantize\n",
    "from coded_computation.master import master\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from pad_and_clean import pad\n",
    "from pad_and_clean import clean_and_scale\n",
    "import time\n",
    "from gen_data import gen_data, gen_nonlinear_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_loss(w,X,y):\n",
    "    #calculates 1-0 prediction error\n",
    "    log_odds = X@w\n",
    "    probs = 1 / (1 + np.exp(-log_odds))\n",
    "    preds = np.where(probs > 0.5, 1,-1)\n",
    "    test_loss = np.mean(preds != y)\n",
    "\n",
    "    return test_loss\n",
    "def plot_3d_bar(data, z):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Define the x, y coordinates and the z heights\n",
    "    _x = np.arange(data.shape[1])\n",
    "    _y = np.arange(data.shape[0])\n",
    "    _x, _y = np.meshgrid(_x, _y)\n",
    "    x, y = _x.ravel(), _y.ravel()\n",
    "\n",
    "    # The z values represent the bar heights\n",
    "    z = np.zeros_like(x)\n",
    "    dz = data.ravel()\n",
    "\n",
    "    # Plot 3D bars\n",
    "    ax.bar3d(x, y, z, 1, 1, dz, shade=True)\n",
    "\n",
    "    # Labels\n",
    "    ax.set_xlabel('w_lvl')\n",
    "    ax.set_ylabel('grd_lvl')\n",
    "    ax.set_zlabel(z)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def run(X,y, filename):\n",
    "    repetitions = 10\n",
    "    func = quant_logistic\n",
    "    G = np.array([\n",
    "        [1, 1, 1, 1, 1, 1, 1],\n",
    "        [-1, -1, -1, 1, 1, 1, 1],\n",
    "        [-1, 1, 1, -1, -1, 1, 1],\n",
    "        [1, -1, -1, -1, -1, 1, 1],\n",
    "        [1, -1, 1, -1, 1, -1, 1],\n",
    "        [-1, 1, -1, -1, 1, -1, 1],\n",
    "        [-1, -1, 1, 1, -1, -1, 1],\n",
    "        [1, 1, -1, 1, -1, -1, 1]\n",
    "    ]).T\n",
    "\n",
    "    stepsize = 0.1\n",
    "    maxiter = 10000\n",
    "\n",
    "    #X,y = gen_nonlinear_data(500, 40, 1)\n",
    "\n",
    "    X, Xt, y, yt = train_test_split(X, y, test_size=0.2)\n",
    "    X, y = pad(X, y, 7)\n",
    "    Xt, yt = pad(Xt, yt, 7)\n",
    "\n",
    "    Master_uncoded = master(X, None, 21)\n",
    "    Master = master(X, G, 3)\n",
    "    times_grid = np.zeros((5, 4))\n",
    "    loss_grid = np.zeros((5, 4))  # Initialize for each dataset\n",
    "    iters_grid = np.zeros((5, 4))\n",
    "    test_loss = np.zeros((5, 4))\n",
    "    test_loss_uncoded = np.zeros((5,4))\n",
    "    uncoded_times = np.zeros((5, 4))\n",
    "    # loss from normal logistic regression\n",
    "    w0 = np.random.uniform(-1, 1, (X.shape[1], 1))\n",
    "    start_time = time.time()\n",
    "    w, num_iters = grdescentnormal(normallogistic, w0, stepsize, maxiter, Master_uncoded, y, X, tolerance=1e-02)\n",
    "    normal_loss = get_loss(w, Xt, yt)\n",
    "    end_time = time.time()\n",
    "    print(f\"loss from unquantized logistic regresison: {normal_loss} on {num_iters} iterations in {end_time - start_time} seconds\")\n",
    "    for i in range(repetitions):\n",
    "        for w_lvl in range(4, 9):\n",
    "            for grd_lvl in range(2, 6):\n",
    "                # logic for calculations\n",
    "                w0 = np.random.uniform(-1, 1, (X.shape[1], 1))\n",
    "\n",
    "                start_time = time.time()\n",
    "                #w, num_iters = grdescentquant(func, w0, stepsize, maxiter, Master, w_lvl, grd_lvl, X, y, filename, tolerance=1e-02)\n",
    "                w, num_iters = grdescentquant(func, w0, stepsize, maxiter, Master, w_lvl, grd_lvl, X, y, filename, 1e-02)\n",
    "                # grdescentquant(func, w, stepsize, maxiter, Master, w_lvl, grd_lvl, X, y, filename, tolerance)\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "                times_grid[w_lvl - 4, grd_lvl - 2] += end_time - start_time\n",
    "                loss_grid[w_lvl - 4, grd_lvl - 2] += get_loss(w, X, y)\n",
    "                iters_grid[w_lvl - 4, grd_lvl - 2] += num_iters\n",
    "                test_loss[w_lvl - 4, grd_lvl - 2] += get_loss(w, Xt, yt)\n",
    "\n",
    "                start_time = time.time()\n",
    "                w, num_iters = grdescentuncoded(uncoded_logistic, w0, stepsize, maxiter, Master_uncoded, w_lvl, grd_lvl, X, y, tolerance=1e-02)\n",
    "                end_time = time.time()\n",
    "                uncoded_times[w_lvl - 4, grd_lvl - 2] += end_time - start_time\n",
    "                test_loss_uncoded[w_lvl - 4, grd_lvl - 2] += get_loss(w,Xt,yt)\n",
    "\n",
    "    test_loss = test_loss/repetitions\n",
    "    loss_grid = loss_grid/repetitions\n",
    "    iters_grid = iters_grid/repetitions\n",
    "    times_grid = times_grid/repetitions\n",
    "    test_loss_uncoded = test_loss_uncoded/repetitions\n",
    "    uncoded_times = uncoded_times/repetitions\n",
    "\n",
    "\n",
    "\n",
    "    print(f\" training loss: \\n{loss_grid}\")\n",
    "    print(f\" avg iterations: \\n{iters_grid}\")\n",
    "    print(f\" test loss: \\n{test_loss}\")\n",
    "    print(f\"run time: \\n{times_grid}\")\n",
    "    print(f\" test loss uncoded: \\n{test_loss_uncoded}\")\n",
    "    print(f\" times uncoded: \\n{uncoded_times}\")\n",
    "    return test_loss, loss_grid, iters_grid, times_grid, test_loss_uncoded, uncoded_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss from unquantized logistic regresison: 0.47619047619047616 on 523 iterations in 0.12335205078125 seconds\n",
      "response, actual \n",
      " [[ -5.53280812  11.45673167]\n",
      " [ -0.75191994 -10.1916842 ]\n",
      " [  5.70759804   9.14573018]\n",
      " [  4.33127206   5.2596469 ]\n",
      " [ 22.1066828    9.7493212 ]] \n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "query does not work: [[-0.95255938]\n [-0.81492589]\n [-0.67729239]\n [-0.5396589 ]\n [-0.4020254 ]\n [-0.26439191]\n [-0.12675841]\n [ 0.01087509]\n [ 0.14850858]\n [ 0.28614208]\n [ 0.42377557]\n [ 0.56140907]\n [ 0.69904256]\n [ 0.83667606]], with error: 279.3797864823014",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m X, y \u001b[38;5;241m=\u001b[39m gen_nonlinear_data(\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m test_loss, loss_grid, iters_grid, times_grid, test_loss_uncoded, uncoded_times \u001b[38;5;241m=\u001b[39m run(X,y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonlinear_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 97\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(X, y, filename)\u001b[0m\n\u001b[1;32m     95\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#w, num_iters = grdescentquant(func, w0, stepsize, maxiter, Master, w_lvl, grd_lvl, X, y, filename, tolerance=1e-02)\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m w, num_iters \u001b[38;5;241m=\u001b[39m grdescentquant(func, w0, stepsize, maxiter, Master, w_lvl, grd_lvl, X, y, filename, \u001b[38;5;241m1e-02\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# grdescentquant(func, w, stepsize, maxiter, Master, w_lvl, grd_lvl, X, y, filename, tolerance)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Desktop/research/low_access-merge_tql/logistic_regression/GrdDscntQuant.py:43\u001b[0m, in \u001b[0;36mgrdescentquant\u001b[0;34m(func, w, stepsize, maxiter, Master, w_lvl, grd_lvl, X, y, filename, tolerance)\u001b[0m\n\u001b[1;32m     31\u001b[0m w, index \u001b[38;5;241m=\u001b[39m quantize(w, w_lvl, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munif\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw-quantization\u001b[39m\u001b[38;5;124m'\u001b[39m: [w_lvl],\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrd-quantization\u001b[39m\u001b[38;5;124m'\u001b[39m: [grd_lvl],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m     } \u001b[38;5;66;03m# for data collection I think\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m loss, gradient \u001b[38;5;241m=\u001b[39m func(w, Master, w_lvl, grd_lvl, \u001b[38;5;28mdict\u001b[39m, X, y, filename, index)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m>\u001b[39m prior_loss:\n\u001b[1;32m     46\u001b[0m     w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m+\u001b[39m stepsize \u001b[38;5;241m*\u001b[39m prior_gradient\n",
      "File \u001b[0;32m~/Desktop/research/low_access-merge_tql/logistic_regression/quant_logistic.py:25\u001b[0m, in \u001b[0;36mquant_logistic\u001b[0;34m(w, Master, w_lvl, grd_lvl, dict, X, y, filename, index)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquant_logistic\u001b[39m(w, Master, w_lvl, grd_lvl, \u001b[38;5;28mdict\u001b[39m, X, y, filename, index):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#y_pred = w.T @ xTr ... now with low access\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m Master\u001b[38;5;241m.\u001b[39muniform_query(w, w_lvl, \u001b[38;5;28mdict\u001b[39m, X, index)\n\u001b[1;32m     26\u001b[0m     vals \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m*\u001b[39m y_pred\n\u001b[1;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mvals)))\n",
      "File \u001b[0;32m~/Desktop/research/low_access-merge_tql/coded_computation/master.py:152\u001b[0m, in \u001b[0;36mmaster.uniform_query\u001b[0;34m(self, w, lvl, dict, X, index)\u001b[0m\n\u001b[1;32m    150\u001b[0m     error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(response \u001b[38;5;241m-\u001b[39m actual)\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse, actual \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mhstack((response\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m], actual\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m])), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery does not work: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39munique(w_flat)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mValueError\u001b[0m: query does not work: [[-0.95255938]\n [-0.81492589]\n [-0.67729239]\n [-0.5396589 ]\n [-0.4020254 ]\n [-0.26439191]\n [-0.12675841]\n [ 0.01087509]\n [ 0.14850858]\n [ 0.28614208]\n [ 0.42377557]\n [ 0.56140907]\n [ 0.69904256]\n [ 0.83667606]], with error: 279.3797864823014"
     ]
    }
   ],
   "source": [
    "X, y = gen_nonlinear_data(500, 40, 1)\n",
    "\n",
    "test_loss, loss_grid, iters_grid, times_grid, test_loss_uncoded, uncoded_times = run(X,y, \"nonlinear_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss from unquantized logistic regresison: 0.09523809523809523 on 277 iterations in 0.06266069412231445 seconds\n",
      " training loss: \n",
      "[[0.39064039 0.43423645 0.37463054 0.43325123]\n",
      " [0.27660099 0.30320197 0.31034483 0.35640394]\n",
      " [0.20073892 0.20862069 0.28029557 0.2453202 ]\n",
      " [0.14827586 0.13300493 0.16477833 0.15615764]\n",
      " [0.11847291 0.09605911 0.14901478 0.16206897]]\n",
      " avg iterations: \n",
      "[[178.9 180.5 161.8 161. ]\n",
      " [187.9 186.  172.7 181.2]\n",
      " [183.1 199.5 189.9 184.3]\n",
      " [208.6 216.5 222.7 221. ]\n",
      " [212.2 245.7 230.9 240.7]]\n",
      " test loss: \n",
      "[[0.36095238 0.46       0.34952381 0.42857143]\n",
      " [0.3447619  0.30952381 0.31619048 0.39714286]\n",
      " [0.24285714 0.22761905 0.31428571 0.26666667]\n",
      " [0.20190476 0.17904762 0.21619048 0.19904762]\n",
      " [0.18380952 0.16095238 0.19809524 0.21142857]]\n",
      "run time: \n",
      "[[2.01193604 2.64873683 2.98601706 3.55165174]\n",
      " [2.20239587 2.87648902 3.23814838 4.04705091]\n",
      " [2.40651972 3.21706297 3.68352432 4.32911398]\n",
      " [2.8330157  3.68087466 4.60713906 5.31738663]\n",
      " [3.23016615 4.54968331 5.16142788 6.29553723]]\n",
      " test loss uncoded: \n",
      "[[0.03619048 0.06761905 0.0352381  0.03428571]\n",
      " [0.0247619  0.06285714 0.03047619 0.06285714]\n",
      " [0.02666667 0.02       0.06571429 0.02857143]\n",
      " [0.01619048 0.01333333 0.02       0.01904762]\n",
      " [0.02666667 0.01809524 0.02380952 0.02761905]]\n",
      " times uncoded: \n",
      "[[0.01747129 0.01439869 0.01430662 0.01440248]\n",
      " [0.01743491 0.01726363 0.01592932 0.0214467 ]\n",
      " [0.01632419 0.0189008  0.0183938  0.01572449]\n",
      " [0.02134352 0.0185056  0.01923368 0.01769688]\n",
      " [0.02966537 0.01965263 0.0204715  0.02019899]]\n"
     ]
    }
   ],
   "source": [
    "X, y = gen_data(500, 40, 1)\n",
    "test_loss1, loss_grid1, iters_grid1, times_grid1, test_loss_uncoded1, uncoded_times1 = run(X,y, \"linear_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(840, 238) (840, 1) (210, 238) (210, 1)\n",
      "loss from unquantized logistic regresison: 0.3952380952380952 on 233 iterations in 0.20769214630126953 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = arff.loadarff('/Users/willem/Downloads/speeddating.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "hill_train_x, hill_test_x, hill_train_y, hill_test_y = clean_and_scale(df, \"match\")\n",
    "hill_train_x, hill_train_y = pad(hill_train_x, hill_train_y, 7)\n",
    "hill_test_x, hill_test_y = pad(hill_test_x, hill_test_y, 7)\n",
    "\n",
    "print(hill_train_x.shape, hill_train_y.shape, hill_test_x.shape, hill_test_y.shape)\n",
    "\n",
    "X = np.vstack((hill_test_x,hill_train_x))\n",
    "y = np.vstack((hill_test_y, hill_train_y))\n",
    "test_loss_real, loss_grid_real, iters_grid_real, times_grid_real, test_loss_uncoded_real, uncoded_times_real  = run(X,y, \"real_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss from unquantized logistic regresison: 0.0761904761904762 on 8416 iterations in 1.4369239807128906 seconds\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": " did not return arithmetic sequence: [-0.39909525433607673, 0.03051861382428145, 0.46013248198463963, 0.9971498171850873, 1.319360218305356, 1.7489740864657142, 2.1785879546260727, 2.6082018227864308, 3.037815690946789, 3.467429559107147, 3.897043427267505, 4.3266572954278635, 4.756271163588222, 5.18588503174858, 5.615498899908938, 6.045112768069296], from vlaues [-0.39909525  0.03051861  0.99714982]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgen_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_seperable_data\n\u001b[1;32m      2\u001b[0m X, y, w \u001b[38;5;241m=\u001b[39m gen_seperable_data(\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m test_loss, loss_grid, iters_grid, times_grid, test_loss_uncoded, uncoded_times \u001b[38;5;241m=\u001b[39m run(X,y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseperable_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 98\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(X, y, filename)\u001b[0m\n\u001b[1;32m     96\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m#w, num_iters = grdescentquant(func, w0, stepsize, maxiter, Master, w_lvl, grd_lvl, X, y, filename, tolerance=1e-02)\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m w, num_iters \u001b[38;5;241m=\u001b[39m grdescentquant(func, w0, stepsize, maxiter, Master, w_lvl, grd_lvl, X, y, filename, \u001b[38;5;241m1e-02\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# grdescentquant(func, w, stepsize, maxiter, Master, w_lvl, grd_lvl, X, y, filename, tolerance)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Desktop/research/low_access-merge_tql/logistic_regression/GrdDscntQuant.py:42\u001b[0m, in \u001b[0;36mgrdescentquant\u001b[0;34m(func, w, stepsize, maxiter, Master, w_lvl, grd_lvl, X, y, filename, tolerance)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m num_iter \u001b[38;5;241m<\u001b[39m maxiter:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw-quantization\u001b[39m\u001b[38;5;124m'\u001b[39m: [w_lvl],\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrd-quantization\u001b[39m\u001b[38;5;124m'\u001b[39m: [grd_lvl],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m         } \u001b[38;5;66;03m# for data collection I think\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     loss, gradient \u001b[38;5;241m=\u001b[39m func(w, Master, w_lvl, grd_lvl, \u001b[38;5;28mdict\u001b[39m, X, y, filename)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m>\u001b[39m prior_loss:\n\u001b[1;32m     45\u001b[0m         w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m+\u001b[39m stepsize \u001b[38;5;241m*\u001b[39m prior_gradient\n",
      "File \u001b[0;32m~/Desktop/research/low_access-merge_tql/logistic_regression/quant_logistic.py:49\u001b[0m, in \u001b[0;36mquant_logistic\u001b[0;34m(w, Master, w_lvl, grd_lvl, dict, X, y, filename)\u001b[0m\n\u001b[1;32m     47\u001b[0m vals \u001b[38;5;241m=\u001b[39m vals\u001b[38;5;241m*\u001b[39my\n\u001b[1;32m     48\u001b[0m alpha \u001b[38;5;241m=\u001b[39m quantize(vals, grd_lvl, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munif\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m uniform_query(alpha, Master, grd_lvl, \u001b[38;5;28mdict\u001b[39m, X)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m     50\u001b[0m record_access(\u001b[38;5;28mdict\u001b[39m, filename)\n\u001b[1;32m     51\u001b[0m gradient \u001b[38;5;241m=\u001b[39m gradient\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/research/low_access-merge_tql/coded_computation/uniform_query.py:30\u001b[0m, in \u001b[0;36muniform_query\u001b[0;34m(w, Master, lvl, dict, X)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# robust index creation is needed\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values) \u001b[38;5;241m!=\u001b[39m expected_len:\n\u001b[0;32m---> 30\u001b[0m     values \u001b[38;5;241m=\u001b[39m impute(values, expected_len, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m     31\u001b[0m index \u001b[38;5;241m=\u001b[39m values\n\u001b[1;32m     32\u001b[0m index \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/research/low_access-merge_tql/coded_computation/impute.py:37\u001b[0m, in \u001b[0;36mimpute\u001b[0;34m(values, expected_len, dict)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m returned incomplete list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_approx_arithmetic_sequence(expected_index, tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m):\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m did not return arithmetic sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, from vlaues \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(expected_index)\n",
      "\u001b[0;31mValueError\u001b[0m:  did not return arithmetic sequence: [-0.39909525433607673, 0.03051861382428145, 0.46013248198463963, 0.9971498171850873, 1.319360218305356, 1.7489740864657142, 2.1785879546260727, 2.6082018227864308, 3.037815690946789, 3.467429559107147, 3.897043427267505, 4.3266572954278635, 4.756271163588222, 5.18588503174858, 5.615498899908938, 6.045112768069296], from vlaues [-0.39909525  0.03051861  0.99714982]"
     ]
    }
   ],
   "source": [
    "from gen_data import gen_seperable_data\n",
    "X, y, w = gen_seperable_data(500, 40)\n",
    "\n",
    "test_loss, loss_grid, iters_grid, times_grid, test_loss_uncoded, uncoded_times = run(X,y, \"seperable_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_bar(data):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    rows, cols = data.shape\n",
    "    x, y = np.meshgrid(np.arange(cols), np.arange(rows))\n",
    "\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "    z = np.zeros_like(x)\n",
    "\n",
    "    dz = data.flatten()\n",
    "\n",
    "    ax.bar3d(x, y, z, 1, 1, dz, shade=True)\n",
    "\n",
    "    ax.set_xlabel('w_lvl')\n",
    "    ax.set_ylabel('grd_lvl')\n",
    "    ax.set_zlabel('z')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_per_query(df):\n",
    "    import numpy as np\n",
    "\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    grouped = df_numeric.groupby(['w-quantization', 'grd-quantization']).mean()\n",
    "\n",
    "    # Reset the index to make plotting easier\n",
    "    grouped = grouped.reset_index()\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    # Setting up the figure and 3D axis\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Data for the bars\n",
    "    x = grouped['w-quantization']\n",
    "    y = grouped['grd-quantization']\n",
    "    z = grouped['time']  # or any other column you wish to visualize\n",
    "\n",
    "    # The bar positions and width\n",
    "    dx = dy = 0.5\n",
    "    dz = z\n",
    "\n",
    "    ax.bar3d(x, y, [0]*len(z), dx, dy, dz, color='b')\n",
    "\n",
    "    # Labeling the axes\n",
    "    ax.set_xlabel('w-quantization')\n",
    "    ax.set_ylabel('grd-quantization')\n",
    "    ax.set_zlabel('Average Time Per Query')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'access_measurements.csv'\n",
    "\n",
    "column_names = [\n",
    "    'w-quantization', 'grd-quantization', 'imputation', 'access',\n",
    "    'query type', 'time', 'stop cond', 'iters'\n",
    "]\n",
    "\n",
    "df_nonlinear = pd.read_csv(file_path, header=None, names=column_names)\n",
    "\n",
    "print(df_nonlinear.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.02223896941294333\n",
      "\n",
      " 0.02223896941294333\n",
      "\n",
      " 0.02223896941294333\n",
      "\n",
      " 0.022238969412943226\n",
      "\n",
      " 0.022238969412943434\n",
      "\n",
      " 0.03335845411941482\n",
      "\n",
      " 0.02223896941294333\n",
      "\n",
      " 0.011119484706471838\n",
      "\n",
      " 0.022238969412943344\n",
      "\n",
      " 0.022238969412943316\n",
      "\n",
      " 0.022238969412943316\n",
      "\n",
      " 0.022238969412943344\n",
      "\n",
      " 0.022238969412943344\n",
      "\n",
      " 0.022238969412943288\n",
      "\n",
      " 0.022238969412943344\n"
     ]
    }
   ],
   "source": [
    "bad_w = [-0.08605444406989449, -0.06381547465695116, -0.04157650524400783, -0.019337535831064498, 0.0029014335818787276, 0.02514040299482216, 0.05849885711423698, 0.08073782652718031\n",
    "         , 0.09185731123365215, 0.1140962806465955, 0.1363352500595388, 0.15857421947248213, 0.18081318888542547, 0.2030521582983688, 0.2252911277113121, 0.24753009712425544]\n",
    "\n",
    "for i in range(1,len(bad_w)):\n",
    "    print(f\"\\n {bad_w[i] - bad_w[i-1]}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
